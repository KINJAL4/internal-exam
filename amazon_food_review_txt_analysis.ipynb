{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R9SKnMh5hOKR",
    "outputId": "8845abee-579a-4a5f-e424-8b5d2f62fe37"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: dask[complete] in /usr/local/lib/python3.7/dist-packages (2.12.0)\n",
      "Requirement already satisfied: toolz>=0.7.3; extra == \"complete\" in /usr/local/lib/python3.7/dist-packages (from dask[complete]) (0.11.1)\n",
      "Requirement already satisfied: partd>=0.3.10; extra == \"complete\" in /usr/local/lib/python3.7/dist-packages (from dask[complete]) (1.1.0)\n",
      "Requirement already satisfied: cloudpickle>=0.2.1; extra == \"complete\" in /usr/local/lib/python3.7/dist-packages (from dask[complete]) (1.3.0)\n",
      "Requirement already satisfied: distributed>=2.0; extra == \"complete\" in /usr/local/lib/python3.7/dist-packages (from dask[complete]) (2021.3.1)\n",
      "Requirement already satisfied: pandas>=0.23.0; extra == \"complete\" in /usr/local/lib/python3.7/dist-packages (from dask[complete]) (1.1.5)\n",
      "Requirement already satisfied: bokeh>=1.0.0; extra == \"complete\" in /usr/local/lib/python3.7/dist-packages (from dask[complete]) (2.3.0)\n",
      "Requirement already satisfied: PyYaml; extra == \"complete\" in /usr/local/lib/python3.7/dist-packages (from dask[complete]) (3.13)\n",
      "Requirement already satisfied: fsspec>=0.6.0; extra == \"complete\" in /usr/local/lib/python3.7/dist-packages (from dask[complete]) (0.8.7)\n",
      "Requirement already satisfied: numpy>=1.13.0; extra == \"complete\" in /usr/local/lib/python3.7/dist-packages (from dask[complete]) (1.19.5)\n",
      "Requirement already satisfied: locket in /usr/local/lib/python3.7/dist-packages (from partd>=0.3.10; extra == \"complete\"->dask[complete]) (0.2.1)\n",
      "Requirement already satisfied: msgpack>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from distributed>=2.0; extra == \"complete\"->dask[complete]) (1.0.2)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from distributed>=2.0; extra == \"complete\"->dask[complete]) (54.2.0)\n",
      "Requirement already satisfied: tblib>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from distributed>=2.0; extra == \"complete\"->dask[complete]) (1.7.0)\n",
      "Requirement already satisfied: click>=6.6 in /usr/local/lib/python3.7/dist-packages (from distributed>=2.0; extra == \"complete\"->dask[complete]) (7.1.2)\n",
      "Requirement already satisfied: tornado>=5; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from distributed>=2.0; extra == \"complete\"->dask[complete]) (5.1.1)\n",
      "Requirement already satisfied: psutil>=5.0 in /usr/local/lib/python3.7/dist-packages (from distributed>=2.0; extra == \"complete\"->dask[complete]) (5.4.8)\n",
      "Requirement already satisfied: zict>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from distributed>=2.0; extra == \"complete\"->dask[complete]) (2.0.0)\n",
      "Requirement already satisfied: sortedcontainers!=2.0.0,!=2.0.1 in /usr/local/lib/python3.7/dist-packages (from distributed>=2.0; extra == \"complete\"->dask[complete]) (2.3.0)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.23.0; extra == \"complete\"->dask[complete]) (2018.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.23.0; extra == \"complete\"->dask[complete]) (2.8.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from bokeh>=1.0.0; extra == \"complete\"->dask[complete]) (3.7.4.3)\n",
      "Requirement already satisfied: pillow>=7.1.0 in /usr/local/lib/python3.7/dist-packages (from bokeh>=1.0.0; extra == \"complete\"->dask[complete]) (7.1.2)\n",
      "Requirement already satisfied: packaging>=16.8 in /usr/local/lib/python3.7/dist-packages (from bokeh>=1.0.0; extra == \"complete\"->dask[complete]) (20.9)\n",
      "Requirement already satisfied: Jinja2>=2.7 in /usr/local/lib/python3.7/dist-packages (from bokeh>=1.0.0; extra == \"complete\"->dask[complete]) (2.11.3)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from fsspec>=0.6.0; extra == \"complete\"->dask[complete]) (3.8.1)\n",
      "Requirement already satisfied: heapdict in /usr/local/lib/python3.7/dist-packages (from zict>=0.1.3->distributed>=2.0; extra == \"complete\"->dask[complete]) (1.0.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=0.23.0; extra == \"complete\"->dask[complete]) (1.15.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=16.8->bokeh>=1.0.0; extra == \"complete\"->dask[complete]) (2.4.7)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2>=2.7->bokeh>=1.0.0; extra == \"complete\"->dask[complete]) (1.1.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->fsspec>=0.6.0; extra == \"complete\"->dask[complete]) (3.4.1)\n"
     ]
    }
   ],
   "source": [
    "%pip install \"dask[complete]\" #installing dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "9W9P7eguhQ7C"
   },
   "outputs": [],
   "source": [
    "import dask.bag as bag #for lazy operations import daskbags\n",
    "import os\n",
    "from dask.diagnostics import ProgressBar # to see the progress of the process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "ODSbrV6FRuKt"
   },
   "outputs": [],
   "source": [
    "raw_text=bag.read_text(\"/content/drive/MyDrive/foods.txt\",encoding='latin1') #read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EMOBC14ng4m0",
    "outputId": "d3f9ffd5-1509-4fcc-c9cc-8da3ae61b176"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5116093"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_text.count().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "HYzVaLHIiKrO"
   },
   "outputs": [],
   "source": [
    "from dask.delayed import delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "iJmgS8iJR4QT"
   },
   "outputs": [],
   "source": [
    "def get_next_buffer_part(file,start_index,span_index=0,blocksize=1000):\n",
    "    file.seek(start_index)\n",
    "    buffer=file.read(blocksize+span_index).decode('latin1')\n",
    "    delimeter_position=buffer.find(\"\\n\\n\")\n",
    "    if delimeter_position==-1:\n",
    "        return get_next_buffer_part(file,start_index,span_index+blocksize)\n",
    "    else:\n",
    "        file.seek(start_index)\n",
    "        return start_index,delimeter_position\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "5BY5iCInR92v"
   },
   "outputs": [],
   "source": [
    "with open(\"/content/drive/MyDrive/foods.txt\",\"rb\") as file_handle:\n",
    "    size = file_handle.seek(0,2) - 1       #Get the total size of the file in bytes\n",
    "    more_data = True                     \n",
    "    output = list()\n",
    "    current_position = next_position = 0\n",
    "    while more_data:\n",
    "        if current_position >= size:\n",
    "            more_data = False\n",
    "        else:\n",
    "            current_position,next_position = get_next_buffer_part(file_handle,current_position,0)\n",
    "            output.append((current_position,next_position))\n",
    "            current_position = current_position + next_position + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "8sC0sqw6TJZw"
   },
   "outputs": [],
   "source": [
    "def get_dict_item(filename,start_index,delimeter_position,encoding='cp1252'):\n",
    "    with open(filename,\"rb\") as file_handle:\n",
    "        file_handle.seek(start_index)\n",
    "        text = file_handle.read(delimeter_position).decode(encoding)\n",
    "        elements = text.strip().split(\"\\n\")\n",
    "        key_value_pairs = [(element.split(\": \")[0], element.split(\": \")[1])\n",
    "                          if len(element.split(\": \")) > 1\n",
    "                          else (\"unknown\",element)\n",
    "                          for element in elements]\n",
    "        return dict(key_value_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "tlb1DFQuSHF9"
   },
   "outputs": [],
   "source": [
    "reviews = bag.from_sequence(output).map(lambda x: get_dict_item(\"/content/drive/MyDrive/foods.txt\",x[0],x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p-iQlYucXwWi",
    "outputId": "3fe04992-d2a1-4321-e6dd-86dbcecee81f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'product/productId': 'B001E4KFG0',\n",
       "  'review/helpfulness': '1/1',\n",
       "  'review/profileName': 'delmartian',\n",
       "  'review/score': '5.0',\n",
       "  'review/summary': 'Good Quality Dog Food',\n",
       "  'review/text': 'I have bought several of the Vitality canned dog food products and have found them all to be of good quality. The product looks more like a stew than a processed meat and it smells better. My Labrador is finicky and she appreciates this product better than  most.',\n",
       "  'review/time': '1303862400',\n",
       "  'review/userId': 'A3SGXH7AUHU8GW'},\n",
       " {'product/productId': 'B00813GRG4',\n",
       "  'review/helpfulness': '0/0',\n",
       "  'review/profileName': 'dll pa',\n",
       "  'review/score': '1.0',\n",
       "  'review/summary': 'Not as Advertised',\n",
       "  'review/text': 'Product arrived labeled as Jumbo Salted Peanuts...the peanuts were actually small sized unsalted. Not sure if this was an error or if the vendor intended to represent the product as \"Jumbo\".',\n",
       "  'review/time': '1346976000',\n",
       "  'review/userId': 'A1D87F6ZCVE5NK'},\n",
       " {'product/productId': 'B000LQOCH0',\n",
       "  'review/helpfulness': '1/1',\n",
       "  'review/profileName': 'Natalia Corres \"Natalia Corres\"',\n",
       "  'review/score': '4.0',\n",
       "  'review/summary': '\"Delight\" says it all',\n",
       "  'review/text': 'This is a confection that has been around a few centuries.  It is a light, pillowy citrus gelatin with nuts - in this case Filberts. And it is cut into tiny squares and then liberally coated with powdered sugar.  And it is a tiny mouthful of heaven.  Not too chewy, and very flavorful.  I highly recommend this yummy treat.  If you are familiar with the story of C.S. Lewis\\' \"The Lion, The Witch, and The Wardrobe\" - this is the treat that seduces Edmund into selling out his Brother and Sisters to the Witch.',\n",
       "  'review/time': '1219017600',\n",
       "  'review/userId': 'ABXLMWJIXXAIN'})"
      ]
     },
     "execution_count": 10,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "i7Lh9VmDX5KG"
   },
   "outputs": [],
   "source": [
    "def fetch_scores(element):\n",
    "    numeric_score = float(element['review/score'])\n",
    "    return numeric_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "V-OcLkiwX_eP"
   },
   "outputs": [],
   "source": [
    "review_scores = reviews.map(fetch_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "MPUg5SR_YEMS"
   },
   "outputs": [],
   "source": [
    "def tag_reviews(element):\n",
    "    if float(element['review/score']) > 3:\n",
    "        element['review/score'] = 'pos'\n",
    "    else:\n",
    "        element['review/score'] = 'neg'\n",
    "    return element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "1IbCgT1xYJt1"
   },
   "outputs": [],
   "source": [
    "reviews = reviews.map(tag_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ERfX3eRPjBKE",
    "outputId": "e98db366-f21b-4a11-ffd2-bc76916a0717"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: contractions in /usr/local/lib/python3.7/dist-packages (0.0.48)\n",
      "Requirement already satisfied: textsearch>=0.0.21 in /usr/local/lib/python3.7/dist-packages (from contractions) (0.0.21)\n",
      "Requirement already satisfied: anyascii in /usr/local/lib/python3.7/dist-packages (from textsearch>=0.0.21->contractions) (0.1.7)\n",
      "Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.7/dist-packages (from textsearch>=0.0.21->contractions) (1.4.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install contractions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oh05kcicSIzw"
   },
   "source": [
    "#importing necessary library for text processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "SLbUiayFYOpI"
   },
   "outputs": [],
   "source": [
    "import pandas as pd                                        #data processing\n",
    "import numpy as np                                         #linear algebra\n",
    "from nltk.tokenize import word_tokenize                    #for words tokenization(break the words)\n",
    "import re                                                  #Regular expressions can contain both special and ordinary characters\n",
    "import nltk\n",
    "from contractions import contractions_dict                 # for removing contractions \n",
    "from nltk.corpus import stopwords\n",
    "from spacy.lang.en.stop_words import STOP_WORDS            #provided stop_words by spacy\n",
    "from itertools import filterfalse \n",
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer                    #for perfect stamming\n",
    "from nltk.corpus import wordnet as wn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "sluHQZ57YT8j"
   },
   "outputs": [],
   "source": [
    "#tokenizing the txt\n",
    "def text_tokenization(x):\n",
    "    x['review/text'] = word_tokenize(x['review/text'])\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "iHJJmdrWYaqQ"
   },
   "outputs": [],
   "source": [
    "tokenized_reviews = reviews.map(text_tokenization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "6nrL5tqyYfM7"
   },
   "outputs": [],
   "source": [
    "# converting all letter in lower case\n",
    "def normalize_tokens(review):\n",
    "    review['review/text'] =  [x.lower() for x in review['review/text']]\n",
    "    return review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "ROh_9M6WYn1U"
   },
   "outputs": [],
   "source": [
    "normalized_reviews = tokenized_reviews.map(normalize_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "WdmftMWWYt0L"
   },
   "outputs": [],
   "source": [
    "# for contraction word expansion\n",
    "def contracted_word_expansion(token):\n",
    "    if token in contractions_dict.keys():\n",
    "        return contractions_dict[token]\n",
    "    else:\n",
    "        return token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "u3bICr_0YyyD"
   },
   "outputs": [],
   "source": [
    "# taking list of token ,doing lazy evaluation using map ,find expansion version of token and give expanded words\n",
    "def contractions_expansion(review):\n",
    "    review['review/text'] = list(map(contracted_word_expansion,review['review/text']))\n",
    "    return review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "NqoSW38iY94x"
   },
   "outputs": [],
   "source": [
    "contracted_reviews = normalized_reviews.map(contractions_expansion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "lf4KOpVOmRZY"
   },
   "outputs": [],
   "source": [
    "regex = r'^@[a-zA-z0-9]|^#[a-zA-Z0-9]|\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*|\\W+|\\d+|<(\"[^\"]*\"|\\'[^\\']*\\'|[^\\'\">])*>|_+|[^\\u0000-\\u007f]+'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "7v-D7JXKnLG-"
   },
   "outputs": [],
   "source": [
    "#check token that is waste word or not using regex\n",
    "def waste_word_or_not(token):\n",
    "    return re.search(regex,token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "D_Dl4EAFnSpX"
   },
   "outputs": [],
   "source": [
    "#\n",
    "def filter_waste_words(review):\n",
    "    review['review/text'] = list(filterfalse(waste_word_or_not,review['review/text']))\n",
    "    return review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "Ok-3cZmwnXUU"
   },
   "outputs": [],
   "source": [
    "filtered_reviews = contracted_reviews.map(filter_waste_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "9fSHyLgfyBzi"
   },
   "outputs": [],
   "source": [
    "def split(review):\n",
    "    review['review/text'] = list(map(lambda x: re.split(regex,x)[0],review['review/text']))\n",
    "    return review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "ZU_tRDjfyHbP"
   },
   "outputs": [],
   "source": [
    "filtered_reviews = filtered_reviews.map(split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OSvPIzVFzXdd",
    "outputId": "b4df8eb8-913e-4dac-8db9-089bcebec437"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "yZ16R_9ayMqr"
   },
   "outputs": [],
   "source": [
    "#collection of stop_word available in english provided by nltk and spacy\n",
    "en_stop_words = list(set(stopwords.words('english')).union(set(STOP_WORDS)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "8mNVwoKUySj8"
   },
   "outputs": [],
   "source": [
    "#check is any token is stopword!! \n",
    "def is_stopword(token):\n",
    "    return not(token in en_stop_words or re.search(r'\\b\\w\\b|[^\\u0000-\\u007f]+|_+|\\W+',token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "iRP2H8bNzkiC"
   },
   "outputs": [],
   "source": [
    "#removing stop_words\n",
    "def stopwords_removal(review):\n",
    "    review['review/text'] = list(filter(is_stopword,review['review/text']))\n",
    "    return review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "iLV7-Odwzq9A"
   },
   "outputs": [],
   "source": [
    "without_stopwords_reviews = filtered_reviews.map(stopwords_removal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "NXy9xfxSzw_Y"
   },
   "outputs": [],
   "source": [
    "#pos_tags(part_of_speech_tags .i.e. verb,adjective,noun,adverb) \n",
    "def get_wnet_pos_tag(treebank_tag):\n",
    "    wn.ensure_loaded()\n",
    "    if treebank_tag[1].startswith('J'):\n",
    "        return (treebank_tag[0],wn.ADJ)\n",
    "    elif treebank_tag[1].startswith('V'):\n",
    "        return (treebank_tag[0],wn.VERB)\n",
    "    elif treebank_tag[1].startswith('N'):\n",
    "        return (treebank_tag[0],wn.NOUN)\n",
    "    elif treebank_tag[1].startswith('R'):\n",
    "        return (treebank_tag[0],wn.ADV)\n",
    "    else:\n",
    "        return (treebank_tag[0],wn.NOUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "ZzMe1fHDz4J9"
   },
   "outputs": [],
   "source": [
    "def get_pos_tag(review):\n",
    "    wn.ensure_loaded()\n",
    "    review['review/text'] = list(map(get_wnet_pos_tag,pos_tag(review['review/text'])))\n",
    "    return review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "ofaB8EAN0DJN"
   },
   "outputs": [],
   "source": [
    "tagged_reviews = without_stopwords_reviews.map(get_pos_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Sr-K52W71AfG",
    "outputId": "1b573a79-f1c3-4309-b16c-2e1688b9d8ea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 38,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mm6iv-oZNoSB"
   },
   "source": [
    "Lemmatizer Uses pos_tags to perform Lemmitization of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "0ddUXzNm0Ilk"
   },
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()  \n",
    "wn.ensure_loaded()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "sRcoVC4r0NdO"
   },
   "outputs": [],
   "source": [
    "def token_lemmatization(token_pos_tuple):\n",
    "    wn.ensure_loaded()\n",
    "    if token_pos_tuple == None:\n",
    "        return \"\"\n",
    "    else:\n",
    "        return lemmatizer.lemmatize(word=token_pos_tuple[0],pos=token_pos_tuple[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "v4L18sBm1XTc"
   },
   "outputs": [],
   "source": [
    "def lemmatization(review):\n",
    "    wn.ensure_loaded()\n",
    "    if len(review['review/text']) > 0:\n",
    "        review['review/text'] = list(map(token_lemmatization,review['review/text']))\n",
    "    else:\n",
    "        review['review/text'] = [\"\"]\n",
    "    return review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "6LpHeeXb1dxA"
   },
   "outputs": [],
   "source": [
    "lemmatized_reviews = tagged_reviews.map(lemmatization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "Kz6IhH_f1iYk"
   },
   "outputs": [],
   "source": [
    "def extract_tokens(review):\n",
    "    return review['review/text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "XDBd4Vav1nAR"
   },
   "outputs": [],
   "source": [
    "extracted_tokens = lemmatized_reviews.map(extract_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "X7A0a8bZ1scS"
   },
   "outputs": [],
   "source": [
    "unique_tokens = extracted_tokens.flatten().distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qnZCqtnF2Y4W",
    "outputId": "516bda82-d749-4b44-b671-bbc56dca5f88"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 46,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nKXetnLdx9iv",
    "outputId": "614250e7-2411-4b83-a402-7657397365c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw to /root/nltk_data...\n",
      "[nltk_data]   Package omw is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 47,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('omw')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "feWHwXSy1z2S",
    "outputId": "43f71864-28d1-4ab1-d0da-68bd4df289ea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed | 48min 37.5s\n"
     ]
    }
   ],
   "source": [
    "#number of unique tokens\n",
    "with ProgressBar():\n",
    "    number_of_tokens = unique_tokens.count().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jfhaRS8-15WU",
    "outputId": "5c3c69f6-8963-4f08-c929-19133b6de6f1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90271"
      ]
     },
     "execution_count": 49,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number_of_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tuWzuc5c4StP",
    "outputId": "975af6a7-396c-44ab-b79e-3600ab97de8e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dask.bag<distinct-aggregate, npartitions=1>"
      ]
     },
     "execution_count": 50,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3XUguF8D4YdJ",
    "outputId": "632987ef-ae8d-4a40-f9a9-8bd7c7dfa56d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed | 48min 35.5s\n"
     ]
    }
   ],
   "source": [
    "with ProgressBar():\n",
    "    tokens_index = list(unique_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "afzQqtOk4ewn"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from collections import OrderedDict\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "fC238SW7zoZP"
   },
   "outputs": [],
   "source": [
    "def join_tokens(list_of_tokens):\n",
    "    return \" \".join(list_of_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "yUGlBcjD4jg2"
   },
   "outputs": [],
   "source": [
    "def compute_tf(review):\n",
    "    D = dict(Counter(review))\n",
    "    non_included = set(tokens_index).difference(set(D.keys()))\n",
    "    D_prime = dict(zip(non_included,list(np.zeros(len(non_included)))))\n",
    "    D_prime.update(D)\n",
    "    full_D = dict(OrderedDict(sorted(D_prime.items())))\n",
    "    print(full_D)\n",
    "    return np.array(full_D.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "etf6A5UN4sWi"
   },
   "outputs": [],
   "source": [
    "tf_vectors = extracted_tokens.map(compute_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "axMl9M1l1q68"
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(vocabulary=unique_tokens)\n",
    "tf_idf_matrix = vectorizer.fit_transform(unique_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "Dl2MBgSt49Mr"
   },
   "outputs": [],
   "source": [
    "from dask import array as dask_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 212
    },
    "id": "NXP7-pSO5Mff",
    "outputId": "cd022d6e-febc-483c-abca-5f9ec9f84ce5"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-2078a4c2e841>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m tf_vector_data = tf_vectors.map(lambda x: dask_array.from_array(x).reshape(1,-1)).reduction(\n\u001b[0;32m----> 2\u001b[0;31m         perpartition=stacker,aggregate=stacker)\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtf_vector_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_vector_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprogress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf_vector_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'stacker' is not defined"
     ]
    }
   ],
   "source": [
    "tf_vector_data = tf_vectors.map(lambda x: dask_array.from_array(x).reshape(1,-1)).reduction(\n",
    "        perpartition=stacker,aggregate=stacker)\n",
    "tf_vector_data = tf_vector_data.compute()\n",
    "progress(tf_vector_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "Imak0yz75KDT"
   },
   "outputs": [],
   "source": [
    "pca = PCA(n_components=5000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 302
    },
    "id": "O2RM54fd4Ygk",
    "outputId": "260113fa-edcc-4365-8420-40113926308e"
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-72-aed14f74c7e4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtf_idf_matrix_reduced\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf_idf_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/decomposition/_pca.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    367\u001b[0m         \u001b[0mC\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mordered\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse\u001b[0m \u001b[0;34m'np.ascontiguousarray'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m         \"\"\"\n\u001b[0;32m--> 369\u001b[0;31m         \u001b[0mU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mV\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    370\u001b[0m         \u001b[0mU\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mU\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_components_\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/decomposition/_pca.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    385\u001b[0m         \u001b[0;31m# This is more informative than the generic one raised by check_array.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 387\u001b[0;31m             raise TypeError('PCA does not support sparse input. See '\n\u001b[0m\u001b[1;32m    388\u001b[0m                             'TruncatedSVD for a possible alternative.')\n\u001b[1;32m    389\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: PCA does not support sparse input. See TruncatedSVD for a possible alternative."
     ]
    }
   ],
   "source": [
    "tf_idf_matrix_reduced = pca.fit_transform(tf_idf_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S_8cRL3tfqp_"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "amazon_food_review_txt_analysis",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
